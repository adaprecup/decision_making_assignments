{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:56:14.072313Z",
     "start_time": "2025-05-22T10:56:14.062254Z"
    }
   },
   "source": [
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import copy"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:56:14.454208Z",
     "start_time": "2025-05-22T10:56:14.450105Z"
    }
   },
   "source": [
    "# make sure pandas is version 1.0 or higher\n",
    "# make sure networkx is verion 2.4 or higher\n",
    "print(pd.__version__)\n",
    "print(nx.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n",
      "3.4.2\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:56:14.928300Z",
     "start_time": "2025-05-22T10:56:14.925039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ema_workbench import (\n",
    "    Policy,\n",
    "    ema_logging,\n",
    "    MultiprocessingEvaluator,\n",
    "    save_results, \n",
    "    load_results,\n",
    "    Samplers\n",
    ")\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:56:15.592002Z",
     "start_time": "2025-05-22T10:56:15.586297Z"
    }
   },
   "cell_type": "code",
   "source": "ema_logging.log_to_stderr(ema_logging.INFO)\n",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger EMA (DEBUG)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### EPA141 Final Assignment: Exploratory Analysis Setup\n",
    "\n",
    "This script configures and runs EMA Workbench experiments to explore the combined impact of external uncertainties and individual policy levers on model outcomes. We create a new function, named run_exploratory_experiments to transform levers into inputs and sample them in a similar fashion to the uncertainities, using Latin Hypercube. It is designed for an initial exploratory analysis phase to identify key drivers of the desired outcomes and to understand the space of possibilities. Literature supporting this approach (number of scenarios, exploring levers as factors):\n",
    "- Bankes, S. (1993). Exploratory Modeling for Policy Analysis. Operations Research, 41(3), 435-449.\n",
    " - Bryant, B. P., & Lempert, R. J. (2010). Thinking inside the box: A participatory, computer-assisted approach to scenario discovery. Technological Forecasting & Social Change, 77(1), 34-49.\n",
    "- Walker, W. E., Marchau, V. A. W. J., & Kwakkel, J. H. (2013). Uncertainty in the framework of Policy Analysis. In W. A. H. Thissen & W. E. Walker (Eds.), Public Policy Analysis:New Developments (pp. 215-261). Springer.\n",
    "- Moallemi, E. A., Kwakkel, J., de Haan, F. J., & Bryan, B. A. (2020). Exploratory modeling for analyzing coupled human-natural systems under uncertainty. Global Environmental Change, 65, 102186.\n",
    "- Course Material: \"Final_Case_Study_Modeling_Steps.docx\" (recommends 1000-3000 simulations).\n",
    "\n",
    "This code is used to generate exploratory experimental data for different problem forumlations of interest to our clients (Group 7 and 21). Every time the problem forumation is changed, a new name should be given for the variable `filename` to ensure you avoid overwriting previous data. "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:56:20.325588Z",
     "start_time": "2025-05-22T10:56:20.319737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def run_exploratory_experiments(problem_formulation_id, num_scenarios, output_filename, sampling_method = None):\n",
    "    \"\"\"\n",
    "    Sets up and runs exploratory experiments where both uncertainties and individual\n",
    "    policy levers are sampled. Saves results using ema_workbench.save_results.\n",
    "\n",
    "    Args:\n",
    "        problem_formulation_id (int): The ID for the problem formulation to load.\n",
    "        sampling_method (str): The name of the sampling method to use.\n",
    "        num_scenarios (int): The number of scenarios (experimental runs) to perform.\n",
    "        output_filename (str): The filename for saving the results (e.g., \"exploratory_results.tar.gz\").\n",
    "    \"\"\"\n",
    "    ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "    # --- 1. Load the Base Model Definition ---\n",
    "    try:\n",
    "        dike_model, _ = get_model_for_problem_formulation(problem_formulation_id)\n",
    "        print(f\"INFO: Loaded model for problem formulation {problem_formulation_id}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load model from problem_formulation.py: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Prepare Model for Exploratory Lever Sampling ---\n",
    "    # For this exploratory run, individual policy levers are treated as factors to be sampled, similar to how uncertainties are sampled.\n",
    "    \n",
    "    exploratory_factors = [] # This list will hold all factors to be sampled.\n",
    "    \n",
    "    # Add original uncertainties\n",
    "    if hasattr(dike_model, 'uncertainties'):\n",
    "        exploratory_factors.extend(copy.deepcopy(dike_model.uncertainties))\n",
    "    else:\n",
    "        print(\"WARNING: dike_model has no 'uncertainties' attribute.\")\n",
    "\n",
    "    # Add original levers to the list of factors to be sampled\n",
    "    # Ensure lever names do not clash with uncertainty names.\n",
    "    original_levers = []\n",
    "    if hasattr(dike_model, 'levers') and dike_model.levers:\n",
    "        original_levers = copy.deepcopy(dike_model.levers)\n",
    "        exploratory_factors.extend(original_levers)\n",
    "        print(f\"INFO: {len(original_levers)} policy levers will be sampled as factors.\")\n",
    "    else:\n",
    "        print(\"WARNING: dike_model has no 'levers' or an empty list of levers.\")\n",
    "\n",
    "    if not exploratory_factors:\n",
    "        print(\"ERROR: No uncertainties or levers found to explore. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # Temporarily assign this combined list to dike_model.uncertainties.\n",
    "    # This instructs EMA Workbench to sample across both original uncertainties and levers.\n",
    "    dike_model.uncertainties = exploratory_factors\n",
    "    \n",
    "    \n",
    "    dike_model.levers = [] \n",
    "\n",
    "    print(f\"INFO: Total number of factors to be sampled: {len(dike_model.uncertainties)}\")\n",
    "\n",
    "    # --- 3. Run Experiments ---\n",
    "    print(f\"INFO: Starting exploratory experiments with {num_scenarios} scenarios...\")\n",
    "\n",
    "    try:\n",
    "        with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "            # 'policies=None' ensures that the workbench does not generate separate policy bundles. Instead, variations in lever settings are part of the 'scenarios'.\n",
    "            # if sampling method is None, then it will run the simulation with default sampling, but we can also set it to use other sampling method to do other analysis (GSA using sobol = Samplers.SOBOL)\n",
    "            if sampling_method is None:\n",
    "                results_tuple = evaluator.perform_experiments(\n",
    "                    scenarios=num_scenarios,\n",
    "                    policies=None,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                results_tuple = evaluator.perform_experiments(\n",
    "                    scenarios=num_scenarios,\n",
    "                    policies=None,\n",
    "                    uncertainty_method= sampling_method\n",
    "                )\n",
    "        print(f\"INFO: Experiment run completed. {len(results_tuple[0])} scenarios were executed.\")\n",
    "        experiments, outcomes = results_tuple\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An error occurred during perform_experiments: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Save Results ---\n",
    "    # Using ema_workbench.save_results is recommended as it preserves data structures.\n",
    "    output_dir = \"../experimental data\"\n",
    "    file_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        try:\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"INFO: Created output directory: {output_dir}\")\n",
    "        except OSError as e:\n",
    "            print(f\"ERROR: Could not create directory {output_dir}: {e}\")\n",
    "            return # Stop if directory cannot be created\n",
    "\n",
    "    try:\n",
    "        save_results(results_tuple, file_path)\n",
    "        print(f\"INFO: Exploratory results successfully saved to: {file_path}\")\n",
    "        # Uncomment the following lines to print the first few rows of the DataFrame\n",
    "        # print (experiments) \n",
    "        # #print column names\n",
    "        # print(\"INFO: Column names in the experiments DataFrame:\")\n",
    "        # print(experiments.columns)\n",
    "        # print(pd.DataFrame(outcomes))\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save results to {file_path}: {e}\")\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#execute the function \n",
    "# Define experiment parameters\n",
    "PROBLEM_FORMULATION_ID = 0 #helps our clients and to test the priorities of other actors\n",
    "# Number of scenarios (runs), aiming for 1000-3000 as per literature and guidelines\n",
    "# due to the function, the arrays get super messed up with around 2000 runs. 1000 is optimal. \n",
    "NUMBER_OF_SCENARIOS = 2000 \n",
    "OUTPUT_FILENAME = \"pf_0_exploratory_runs_levers_as_factors.tar.gz\"\n",
    "SAMPLING_METHOD = None\n",
    "\n",
    "run_exploratory_experiments(PROBLEM_FORMULATION_ID, NUMBER_OF_SCENARIOS, OUTPUT_FILENAME, SAMPLING_METHOD)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### The following cells will be used to generate experimental runs with candidate policies\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# defining specific = template policies for later \n",
    "# for example, policy 1 is about extra protection in upper boundary\n",
    "# policy 2 is about extra protection in lower boundary\n",
    "# policy 3 is extra protection in random locations\n",
    "\n",
    "\n",
    "def get_do_nothing_dict():\n",
    "    return {l.name: 0 for l in dike_model.levers}\n",
    "\n",
    "\n",
    "policies = [\n",
    "    Policy(\n",
    "        \"policy 1\",\n",
    "        **dict(\n",
    "            get_do_nothing_dict(),\n",
    "            **{\"0_RfR 0\": 1, \"0_RfR 1\": 1, \"0_RfR 2\": 1, \"A.1_DikeIncrease 0\": 5}\n",
    "        )\n",
    "    ),\n",
    "    Policy(\n",
    "        \"policy 2\",\n",
    "        **dict(\n",
    "            get_do_nothing_dict(),\n",
    "            **{\"4_RfR 0\": 1, \"4_RfR 1\": 1, \"4_RfR 2\": 1, \"A.5_DikeIncrease 0\": 5}\n",
    "        )\n",
    "    ),\n",
    "    Policy(\n",
    "        \"policy 3\",\n",
    "        **dict(\n",
    "            get_do_nothing_dict(),\n",
    "            **{\"1_RfR 0\": 1, \"2_RfR 1\": 1, \"3_RfR 2\": 1, \"A.3_DikeIncrease 0\": 5}\n",
    "        )\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# pass the policies list to EMA workbench experiment runs\n",
    "n_scenarios = 100\n",
    "with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(n_scenarios, policies)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "experiments, outcomes = results"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# only works because we have scalar outcomes\n",
    "pd.DataFrame(outcomes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
