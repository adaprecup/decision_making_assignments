{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:57:33.267119Z",
     "start_time": "2025-06-02T14:57:33.150802Z"
    }
   },
   "source": [
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import copy\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:57:33.644204Z",
     "start_time": "2025-06-02T14:57:33.641467Z"
    }
   },
   "source": [
    "# make sure pandas is version 1.0 or higher\n",
    "# make sure networkx is verion 2.4 or higher\n",
    "print(pd.__version__)\n",
    "print(nx.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n",
      "3.4.2\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:57:35.059939Z",
     "start_time": "2025-06-02T14:57:34.440073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ema_workbench import (\n",
    "    Policy,\n",
    "    ema_logging,\n",
    "    MultiprocessingEvaluator,\n",
    "    save_results, \n",
    "    load_results,\n",
    "    Samplers\n",
    ")\n",
    "from problem_formulation import get_model_for_problem_formulation\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/precupada/decision_making_assignments/venv/lib/python3.13/site-packages/ema_workbench/em_framework/__init__.py:101: UserWarning: ipyparallel not installed - IpyparalleEvaluator not available\n",
      "  warnings.warn(\"ipyparallel not installed - IpyparalleEvaluator not available\")\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:57:35.170523Z",
     "start_time": "2025-06-02T14:57:35.164556Z"
    }
   },
   "cell_type": "code",
   "source": "ema_logging.log_to_stderr(ema_logging.INFO)\n",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger EMA (DEBUG)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### EPA141 Final Assignment: Exploratory Analysis Setup\n",
    "\n",
    "This script configures and runs EMA Workbench experiments to explore the combined impact of external uncertainties and individual policy levers on model outcomes. We create a new function, named run_exploratory_experiments to transform levers into inputs and sample them in a similar fashion to the uncertainities, using Latin Hypercube. It is designed for an initial exploratory analysis phase to identify key drivers of the desired outcomes and to understand the space of possibilities. Literature supporting this approach (number of scenarios, exploring levers as factors):\n",
    "- Bankes, S. (1993). Exploratory Modeling for Policy Analysis. Operations Research, 41(3), 435-449.\n",
    " - Bryant, B. P., & Lempert, R. J. (2010). Thinking inside the box: A participatory, computer-assisted approach to scenario discovery. Technological Forecasting & Social Change, 77(1), 34-49.\n",
    "- Walker, W. E., Marchau, V. A. W. J., & Kwakkel, J. H. (2013). Uncertainty in the framework of Policy Analysis. In W. A. H. Thissen & W. E. Walker (Eds.), Public Policy Analysis:New Developments (pp. 215-261). Springer.\n",
    "- Moallemi, E. A., Kwakkel, J., de Haan, F. J., & Bryan, B. A. (2020). Exploratory modeling for analyzing coupled human-natural systems under uncertainty. Global Environmental Change, 65, 102186.\n",
    "- Course Material: \"Final_Case_Study_Modeling_Steps.docx\" (recommends 1000-3000 simulations).\n",
    "\n",
    "This code is used to generate exploratory experimental data for different problem forumlations of interest to our clients (Group 7 and 21). Every time the problem forumation is changed, a new name should be given for the variable `filename` to ensure you avoid overwriting previous data. "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:57:37.383620Z",
     "start_time": "2025-06-02T14:57:37.372427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_exploratory_experiments(problem_formulation_id, num_scenarios, output_filename, sampling_method=None, active_rfr_dikes=None):\n",
    "    \"\"\"\n",
    "    Sets up and runs exploratory experiments where both uncertainties and individual\n",
    "    policy levers are sampled. Optionally turns RfR levers ON for specific dike rings.\n",
    "    \n",
    "    Args:\n",
    "        problem_formulation_id (int): ID for the problem formulation to load.\n",
    "        num_scenarios (int): Number of scenarios (experimental runs) to perform.\n",
    "        output_filename (str): Filename for saving the results (e.g., \"results.tar.gz\").\n",
    "        sampling_method (str, optional): Name of sampling method (e.g., Samplers.SOBOL).\n",
    "        active_rfr_dikes (list of str, optional): List of dike names (e.g., [\"A.5\"]) to activate RfR for.\n",
    "    \"\"\"\n",
    "    ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "    # --- 1. Load the Base Model Definition ---\n",
    "    try:\n",
    "        from problem_formulation import get_model_for_problem_formulation\n",
    "        dike_model, _ = get_model_for_problem_formulation(problem_formulation_id)\n",
    "        print(f\"INFO: Loaded model for problem formulation {problem_formulation_id}.\")\n",
    "\n",
    "        # Force RfR levers ON for selected dikes\n",
    "        if active_rfr_dikes:\n",
    "            modified_levers = []\n",
    "            for lev in dike_model.levers:\n",
    "                if \"RfR\" in lev.name and any(dike in lev.name for dike in active_rfr_dikes):\n",
    "                    modified_levers.append(IntegerParameter(lev.name, 1, 1))  # Always ON\n",
    "                else:\n",
    "                    modified_levers.append(lev)\n",
    "            dike_model.levers = modified_levers\n",
    "            print(f\"INFO: RfR levers activated for dike(s): {active_rfr_dikes}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load model from problem_formulation.py: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Prepare Model for Exploratory Lever Sampling ---\n",
    "    exploratory_factors = []\n",
    "\n",
    "    if hasattr(dike_model, 'uncertainties'):\n",
    "        exploratory_factors.extend(copy.deepcopy(dike_model.uncertainties))\n",
    "    else:\n",
    "        print(\"WARNING: dike_model has no 'uncertainties' attribute.\")\n",
    "\n",
    "    original_levers = []\n",
    "    if hasattr(dike_model, 'levers') and dike_model.levers:\n",
    "        original_levers = copy.deepcopy(dike_model.levers)\n",
    "        exploratory_factors.extend(original_levers)\n",
    "        print(f\"INFO: {len(original_levers)} policy levers will be sampled as factors.\")\n",
    "    else:\n",
    "        print(\"WARNING: dike_model has no 'levers' or an empty list of levers.\")\n",
    "\n",
    "    if not exploratory_factors:\n",
    "        print(\"ERROR: No uncertainties or levers found to explore. Aborting.\")\n",
    "        return\n",
    "\n",
    "    dike_model.uncertainties = exploratory_factors\n",
    "    dike_model.levers = []\n",
    "\n",
    "    print(f\"INFO: Total number of factors to be sampled: {len(dike_model.uncertainties)}\")\n",
    "\n",
    "    # --- 3. Run Experiments ---\n",
    "    print(f\"INFO: Starting exploratory experiments with {num_scenarios} scenarios...\")\n",
    "\n",
    "    try:\n",
    "        with MultiprocessingEvaluator(dike_model) as evaluator:\n",
    "            if sampling_method is None:\n",
    "                results_tuple = evaluator.perform_experiments(\n",
    "                    scenarios=num_scenarios,\n",
    "                    policies=None,\n",
    "                )\n",
    "            else:\n",
    "                results_tuple = evaluator.perform_experiments(\n",
    "                    scenarios=num_scenarios,\n",
    "                    policies=None,\n",
    "                    uncertainty_method=sampling_method\n",
    "                )\n",
    "        print(f\"INFO: Experiment run completed. {len(results_tuple[0])} scenarios were executed.\")\n",
    "        experiments, outcomes = results_tuple\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An error occurred during perform_experiments: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Save Results ---\n",
    "    output_dir = \"../experimental data\"\n",
    "    file_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        try:\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"INFO: Created output directory: {output_dir}\")\n",
    "        except OSError as e:\n",
    "            print(f\"ERROR: Could not create directory {output_dir}: {e}\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        save_results(results_tuple, file_path)\n",
    "        print(f\"INFO: Exploratory results successfully saved to: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save results to {file_path}: {e}\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:23:02.566652Z",
     "start_time": "2025-06-02T15:21:34.322555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#execute the function \n",
    "# Define experiment parameters\n",
    "PROBLEM_FORMULATION_ID = 5 #helps our clients and to test the priorities of other actors\n",
    "# Number of scenarios (runs), aiming for 1000-3000 as per literature and guidelines\n",
    "# due to the function, the arrays get super messed up with around 2000 runs. 1000 is optimal. \n",
    "NUMBER_OF_SCENARIOS = 2000 \n",
    "OUTPUT_FILENAME = \"pf_5_exploratory_runs.tar.gz\"\n",
    "SAMPLING_METHOD = None\n",
    "active_rfr_dikes = [\"A.1\", \"A.4\", \"A.3\"]  # dikes RWS wants to activate RfR for\n",
    "\n",
    "\n",
    "run_exploratory_experiments(PROBLEM_FORMULATION_ID, NUMBER_OF_SCENARIOS, OUTPUT_FILENAME, SAMPLING_METHOD,  active_rfr_dikes=None)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Loaded model for problem formulation 5.\n",
      "INFO: 31 policy levers will be sampled as factors.\n",
      "INFO: Total number of factors to be sampled: 50\n",
      "INFO: Starting exploratory experiments with 2000 scenarios...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/precupada/decision_making_assignments/venv/lib/python3.13/site-packages/ema_workbench/em_framework/__init__.py:101: UserWarning: ipyparallel not installed - IpyparalleEvaluator not available\n",
      "  warnings.warn(\"ipyparallel not installed - IpyparalleEvaluator not available\")\n",
      "/Users/precupada/decision_making_assignments/venv/lib/python3.13/site-packages/ema_workbench/em_framework/__init__.py:101: UserWarning: ipyparallel not installed - IpyparalleEvaluator not available\n",
      "  warnings.warn(\"ipyparallel not installed - IpyparalleEvaluator not available\")\n",
      "/Users/precupada/decision_making_assignments/venv/lib/python3.13/site-packages/ema_workbench/em_framework/__init__.py:101: UserWarning: ipyparallel not installed - IpyparalleEvaluator not available\n",
      "  warnings.warn(\"ipyparallel not installed - IpyparalleEvaluator not available\")\n",
      "/Users/precupada/decision_making_assignments/venv/lib/python3.13/site-packages/ema_workbench/em_framework/__init__.py:101: UserWarning: ipyparallel not installed - IpyparalleEvaluator not available\n",
      "  warnings.warn(\"ipyparallel not installed - IpyparalleEvaluator not available\")\n",
      "/Users/precupada/decision_making_assignments/venv/lib/python3.13/site-packages/ema_workbench/em_framework/__init__.py:101: UserWarning: ipyparallel not installed - IpyparalleEvaluator not available\n",
      "  warnings.warn(\"ipyparallel not installed - IpyparalleEvaluator not available\")\n",
      "/Users/precupada/decision_making_assignments/venv/lib/python3.13/site-packages/ema_workbench/em_framework/__init__.py:101: UserWarning: ipyparallel not installed - IpyparalleEvaluator not available\n",
      "  warnings.warn(\"ipyparallel not installed - IpyparalleEvaluator not available\")\n",
      "/Users/precupada/decision_making_assignments/venv/lib/python3.13/site-packages/ema_workbench/em_framework/__init__.py:101: UserWarning: ipyparallel not installed - IpyparalleEvaluator not available\n",
      "  warnings.warn(\"ipyparallel not installed - IpyparalleEvaluator not available\")\n",
      "/Users/precupada/decision_making_assignments/venv/lib/python3.13/site-packages/ema_workbench/em_framework/__init__.py:101: UserWarning: ipyparallel not installed - IpyparalleEvaluator not available\n",
      "  warnings.warn(\"ipyparallel not installed - IpyparalleEvaluator not available\")\n",
      "[MainProcess/INFO] pool started with 8 workers\n",
      "[MainProcess/INFO] performing 2000 scenarios * 1 policies * 1 model(s) = 2000 experiments\n",
      "100%|██████████████████████████████████████| 2000/2000 [01:19<00:00, 25.10it/s]\n",
      "[MainProcess/INFO] experiments finished\n",
      "[MainProcess/INFO] terminating pool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Experiment run completed. 2000 scenarios were executed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] results saved successfully to /Users/precupada/decision_making_assignments/final assignment EPA141/experimental data/pf_5_exploratory_runs.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Exploratory results successfully saved to: ../experimental data/pf_5_exploratory_runs.tar.gz\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
